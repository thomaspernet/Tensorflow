{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is an Autoencoder?\n",
    "\n",
    "An autoencoder is a great tool to recreate an input. In a simple word,\n",
    "the machine takes, let's say an image, and can produce a closely related\n",
    "picture. The input in this kind of neural network is unlabelled, meaning\n",
    "the network is capable of learning without supervision. More precisely,\n",
    "the input is encoded by the network to focus only on the most critical\n",
    "feature. This is one of the reasons why autoencoder is popular for\n",
    "dimensionality reduction. Besides, autoencoders can be used to produce\n",
    "**generative learning models**. For example, the neural network can be\n",
    "trained with a set of faces and then can produce new faces.\n",
    "\n",
    "In this tutorial, you will learn what an autoencoder is and how to build\n",
    "such type of network with Tensorflow to reconstruct an image.\n",
    "\n",
    "## How does it work?\n",
    "\n",
    "The purpose of an autoencoder is to produce an approximation of the\n",
    "input by focusing only on the essential features. You may think why not\n",
    "merely learn how to copy and paste the input to produce the output. In\n",
    "fact, an autoencoder is a set of constraints that force the network to\n",
    "learn new ways to represent the data, different from merely copying the\n",
    "output.\n",
    "\n",
    "A typical autoencoder is defined with an input, an internal\n",
    "representation and an output (an approximation of the input). The\n",
    "learning occurs in the layers attached to the internal representation.\n",
    "In fact, there are two main blocks of layers which looks like a\n",
    "traditional neural network. The slight difference is the layer\n",
    "containing the output must be equal to the input. In the picture below,\n",
    "the original input goes into the first block called the **encoder**.\n",
    "This internal representation compresses (reduces) the size of the input.\n",
    "In the second block occurs the reconstruction of the input. This is the\n",
    "decoding phase.\n",
    "\n",
    "![](https://github.com/thomaspernet/Tensorflow/blob/master/tensorflow/22_autoencoder_v4_files/image001.png)\n",
    "\n",
    "The model will update the weights by minimizing the loss function. The\n",
    "model is penalized if the reconstruction output is different from the\n",
    "input.\n",
    "\n",
    "Concretely, imagine a picture with a size of 50x50 (i.e., 250 pixels)\n",
    "and a neural network with just one hidden layer composed of one hundred\n",
    "neurons. The learning is done on a feature map which is two times\n",
    "smaller than the input. It means the network needs to find a way to\n",
    "reconstruct 250 pixels with only a vector of neurons equal to 100.\n",
    "\n",
    "## Stacked Autoencoder Example\n",
    "\n",
    "In this tutorial, you will learn how to use a stacked autoencoder. The\n",
    "architecture is similar to a traditional neural network. The input goes\n",
    "to a hidden layer in order to be *compressed*, or reduce its size, and\n",
    "then reaches the reconstruction layers. The objective is to produce an\n",
    "output image as close as the original. The model has to learn a way to\n",
    "achieve its task under a set of constraints, that is, with a lower\n",
    "dimension.\n",
    "\n",
    "Nowadays, autoencoders are mainly used to denoise an image. Imagine an\n",
    "image with scratches; a human is still able to recognize the content.\n",
    "The idea of denoising autoencoder is to add noise to the picture to\n",
    "force the network to learn the pattern behind the data.\n",
    "\n",
    "The other useful family of autoencoder is variational autoencoder. This\n",
    "type of network can generate new images. Imagine you train a network\n",
    "with the image of a man; such a network can produce new faces.\n",
    "\n",
    "## Build an Autoencoder with TensorFlow\n",
    "\n",
    "In this tutorial, you will learn how to build a stacked autoencoder to\n",
    "reconstruct an image.\n",
    "\n",
    "You will use the [CIFAR-10\n",
    "dataset](https://www.cs.toronto.edu/~kriz/cifar.html) which contains\n",
    "60000 32x32 color images. The dataset is already split between 50000\n",
    "images for training and 10000 for testing. There are up to ten classes:\n",
    "\n",
    "-   Airplane\n",
    "\n",
    "-   Automobile\n",
    "\n",
    "-   Bird\n",
    "\n",
    "-   Cat\n",
    "\n",
    "-   Deer\n",
    "\n",
    "-   Dog\n",
    "\n",
    "-   Frog\n",
    "\n",
    "-   Horse\n",
    "\n",
    "-   Ship\n",
    "\n",
    "-   Truck\n",
    "\n",
    "You need download the images in this [URL](https://www.cs.toronto.edu/\\~kriz/cifar.html) and unzip it. The folder\n",
    "*for-10-batches-py* contains five batches of data with 10000 images each\n",
    "in a random order.\n",
    "\n",
    "Before you build and train your model, you need to apply some data\n",
    "processing. You will proceed as follow:\n",
    "\n",
    "1.  Import the data\n",
    "\n",
    "2.  Convert the data to black and white format\n",
    "\n",
    "3.  Append all the batches\n",
    "\n",
    "4.  Construct the training dataset\n",
    "\n",
    "5.  Construct an image visualizer\n",
    "\n",
    "## Image preprocessing\n",
    "\n",
    "**Step 1:** Import the data.\n",
    "\n",
    "According to the official website, you can upload the data with the\n",
    "following code. The code will load the data in a dictionary with the\n",
    "**data** and the **label**. Note that the code is a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Thomas/anaconda3/envs/hello-tf/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='latin1')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Convert the data to black and white format\n",
    "\n",
    "For simplicity, you will convert the data to a grayscale. That is, with\n",
    "only one dimension against three for colors image. Most of the neural\n",
    "network works only with one dimension input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grayscale(im):\n",
    "    return im.reshape(im.shape[0], 3, 32, 32).mean(1).reshape(im.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Append all the batches\n",
    "\n",
    "Now that both functions are created and the dataset loaded, you can\n",
    "write a loop to append the data in memory. If you check carefully, the\n",
    "unzip file with the data is named `data_batch_` with a number from 1 to\n",
    "5. You can loop over the files and append it to `data`.\n",
    "\n",
    "When this step is done, you convert the colours data to a gray scale\n",
    "format. As you can see, the shape of the data is 50000 and 1024. The\n",
    "32\\*32 pixels are now flatten to 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Load the data into memory\n",
    "data, labels = [], []\n",
    "## Loop over the b\n",
    "for i in range(1, 6):\n",
    "    filename = '/Users/Thomas/Downloads/cifar-10-batches-py/data_batch_' + str(i)\n",
    "    open_data = unpickle(filename)\n",
    "    if len(data) > 0:\n",
    "        data = np.vstack((data, open_data['data']))\n",
    "        labels = np.hstack((labels, open_data['labels']))\n",
    "    else:\n",
    "        data = open_data['data']\n",
    "        labels = open_data['labels']\n",
    "\n",
    "data = grayscale(data)\n",
    "x = np.matrix(data)\n",
    "y = np.array(labels)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Construct the training dataset\n",
    "\n",
    "To make the training faster and easier, you will train a model on the\n",
    "horse images only. The horses are the seventh class in the label data.\n",
    "As mentioned in the documentation of the CIFAR-10 dataset, each class\n",
    "contains 5000 images. You can print the shape of the data to confirm\n",
    "there are 5.000 images with 1024 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 1024)\n"
     ]
    }
   ],
   "source": [
    "horse_i = np.where(y == 7)[0]\n",
    "horse_x = x[horse_i]\n",
    "print(np.shape(horse_x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5:** Construct an image visualizer\n",
    "\n",
    "Finally, you construct a function to plot the images. You will need this\n",
    "function to print the reconstructed image from the autoencoder.\n",
    "\n",
    "An easy way to print images is to use the object `imshow` from the\n",
    "`matplotlib` library. Note that, you need to convert the shape of the\n",
    "data from 1024 to 32\\*32 (i.e. format of an image). *\\# To plot pretty\n",
    "figures*\\\n",
    "%matplotlib inline\\\n",
    "import matplotlib\\\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image, shape=[32, 32], cmap = \"Greys_r\"):\n",
    "    plt.imshow(image.reshape(shape), cmap=cmap,interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function takes 3 arguments:\n",
    "\n",
    "-   Image:  the input\n",
    "-   Shape: list, the dimension of the image\n",
    "-   Cmap:choose the color map. By default, grey\n",
    "\n",
    "You can try to plot the first image in the dataset. You should see a man\n",
    "on a horse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_image(horse_x[1], shape=[32, 32], cmap = \"Greys_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/thomaspernet/Tensorflow/blob/master/tensorflow/22_autoencoder_v4_files/image002.png)\n",
    "\n",
    "## Set Dataset Estimator\n",
    "\n",
    "All right, now that the dataset is ready to use, you can start to use\n",
    "Tensorflow. Before to build the model, let's use the `Dataset` estimator\n",
    "of Tensorflow to feed the network.\n",
    "\n",
    "You will build a `Dataset` with TensorFlow estimator. To refresh your\n",
    "mind, you need to use:\n",
    "\n",
    "-   `from_tensor_slices`\n",
    "\n",
    "-   `repeat`\n",
    "\n",
    "-   `batch`\n",
    "\n",
    "The full code to build the dataset is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "n_inputs = 32 * 32\n",
    "BATCH_SIZE = 1\n",
    "batch_size = tf.placeholder(tf.int64)\n",
    "\n",
    "# using a placeholder\n",
    "x = tf.placeholder(tf.float32, shape=[None,n_inputs])\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x).repeat().batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, `x` is a placeholder with the following shape:\n",
    "\n",
    "-   `[None,n_inputs]`: Set to `None` because the number of image feed to\n",
    "    the network is equal to the batch size.\n",
    "\n",
    "for details, please refer to the tutorial on linear\n",
    "regression.\n",
    "\n",
    "After that, you need to create the iterator. Without this line of code,\n",
    "no data will go through the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = dataset.make_initializable_iterator() # create the iterator\n",
    "features = iter.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the pipeline is ready, you can check if the first image is the\n",
    "same as before (i.e., a man on a horse).\n",
    "\n",
    "You set the batch size to 1 because you only want to feed the dataset\n",
    "with one image. You can see the dimension of the data with\n",
    "`print(sess.run(features).shape)`. It is equal to (1, 1024). 1 means\n",
    "only one image with 1024 is feed each. If the batch size is set to two,\n",
    "then two images will go through the pipeline. (Don't change the batch\n",
    "size. Otherwise, it will throw an error. Only one image at a time can go\n",
    "to the function `plot_image()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1024)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Parameters\n",
    "n_inputs = 32 * 32\n",
    "BATCH_SIZE = 1\n",
    "batch_size = tf.placeholder(tf.int64)\n",
    "\n",
    "# using a placeholder\n",
    "x = tf.placeholder(tf.float32, shape=[None,n_inputs])\n",
    "## Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x).repeat().batch(batch_size)\n",
    "iter = dataset.make_initializable_iterator() # create the iterator\n",
    "features = iter.get_next()\n",
    "\n",
    "## Print the image\n",
    "with tf.Session() as sess:\n",
    "    # feed the placeholder with data\n",
    "    sess.run(iter.initializer, feed_dict={x: horse_x,\n",
    "                                         batch_size: BATCH_SIZE}) \n",
    "    print(sess.run(features).shape) \n",
    "    plot_image(sess.run(features), shape=[32, 32], cmap = \"Greys_r\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/thomaspernet/Tensorflow/blob/master/tensorflow/22_autoencoder_v4_files/image002.png)\n",
    "\n",
    "## Build the network\n",
    "\n",
    "It is time to construct the network. You will train a stacked\n",
    "autoencoder, that is, a network with multiple hidden layers.\n",
    "\n",
    "Your network will have one input layers with 1024 points, i.e., 32x32,\n",
    "the shape of the image.\n",
    "\n",
    "The encoder block will have one top hidden layer with 300 neurons, a\n",
    "central layer with 150 neurons. The decoder block is symmetric to the\n",
    "encoder. You can visualize the network in the picture below. Note that\n",
    "you can change the values of hidden and central layers.\n",
    "\n",
    "![](https://github.com/thomaspernet/Tensorflow/blob/master/tensorflow/22_autoencoder_v4_files/image003.png)\n",
    "\n",
    "Building an autoencoder is very similar to any other deep learning\n",
    "model.\n",
    "\n",
    "You will construct the model following these steps:\n",
    "\n",
    "1.  Define the parameters\n",
    "\n",
    "2.  Define the layers\n",
    "\n",
    "3.  Define the architecture\n",
    "\n",
    "4.  Define the optimization\n",
    "\n",
    "5.  Run the model\n",
    "\n",
    "6.  Evaluate the model\n",
    "\n",
    "In the previous section, you learned how to create a pipeline to feed\n",
    "the model, so there is no need to create once more the dataset. You will\n",
    "construct an autoencoder with four layers. You use the Xavier\n",
    "initialization. This is a technique to set the initial weights equal to\n",
    "the variance of both the input and output. Finally, you use the elu\n",
    "activation function. You regularize the loss function with L2\n",
    "regularizer.\n",
    "\n",
    "**Step 1:** Define the parameters\n",
    "\n",
    "The first step implies to define the number of neurons in each layer,\n",
    "the learning rate and the hyperparameter of the regularizer.\n",
    "\n",
    "Before that, you import the function partially. It is a better method to\n",
    "define the parameters of the dense layers. The code below defines the\n",
    "values of the autoencoder architecture. As listed before, the\n",
    "autoencoder has two layers, with 300 neurons in the first layers and 150\n",
    "in the second layers. Their values are stored in `n_hidden_1` and\n",
    "`n_hidden_2`.\n",
    "\n",
    "You need to define the learning rate and the L2 hyperparameter. The\n",
    "values are stored in `learning_rate` and `l2_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "## Encoder\n",
    "n_hidden_1 = 300\n",
    "n_hidden_2 = 150  # codings\n",
    "\n",
    "## Decoder\n",
    "n_hidden_3 = n_hidden_1\n",
    "n_outputs = n_inputs\n",
    "\n",
    "learning_rate = 0.01\n",
    "l2_reg = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Xavier initialization technique is called with the object\n",
    "`xavier_initializer` from the estimator `contrib`. In the same\n",
    "estimator, you can add the regularizer with `l2_regularizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the Xavier initialization\n",
    "xav_init =  tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "## Define the L2 regularizer\n",
    "l2_regularizer = tf.contrib.layers.l2_regularizer(l2_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Define the layers\n",
    "\n",
    "All the parameters of the dense layers have been set; you can pack\n",
    "everything in the variable `dense_layer` by using the object `partial`.\n",
    "`dense_layer`` which` uses the ELU activation, Xavier initialization,\n",
    "and L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the dense layer\n",
    "dense_layer = partial(tf.layers.dense,\n",
    "                         activation=tf.nn.elu,\n",
    "                         kernel_initializer=xav_init,\n",
    "                         kernel_regularizer=l2_regularizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Define the architecture\n",
    "\n",
    "If you look at the picture of the architecture, you note that the\n",
    "network stacks three layers with an output layer. In the code below, you\n",
    "connect the appropriate layers. For instance, the first layer computes\n",
    "the dot product between the inputs matrice `features` and the matrices\n",
    "containing the 300 weights. After the dot product is computed, the\n",
    "output goes to the Elu activation function. The output becomes the input\n",
    "of the next layer, that is why you use it to compute `hidden_2` and so\n",
    "on. The matrices multiplication are the same for each layer because you\n",
    "use the same activation function. Note that the last layer, `outputs`,\n",
    "does not apply an activation function. It makes sense because this is\n",
    "the reconstructed input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the mat mul\n",
    "hidden_1 = dense_layer(features, n_hidden_1)\n",
    "hidden_2 = dense_layer(hidden_1, n_hidden_2)\n",
    "hidden_3 = dense_layer(hidden_2, n_hidden_3)\n",
    "outputs = dense_layer(hidden_3, n_outputs, activation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Define the optimization\n",
    "\n",
    "The last step is to construct the optimizer. You use the Mean Square\n",
    "Error as a loss function. If you recall the tutorial on linear\n",
    "regression, you know that the MSE is computed with the difference\n",
    "between the predicted output and the real label. Here, the label is the\n",
    "feature because the model tries to reconstruct the input. Therefore, you\n",
    "want the mean of the sum of difference of the square between predicted\n",
    "output and input. With TensorFlow, you can code the loss function as\n",
    "follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.square(outputs - features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you need to optimize the loss function. You use Adam optimizer to\n",
    "compute the gradients. The objective function is to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimize\n",
    "loss = tf.reduce_mean(tf.square(outputs - features))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train  = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more setting before training the model. You want to use a batch size\n",
    "of 150, that is, feed the pipeline with 150 images each iteration. You\n",
    "need to compute the number of iterations manually. This is trivial to\n",
    "do:\n",
    "\n",
    "If you want to pass 150 images each time and you know there are 5000\n",
    "images in the dataset, the number of iterations is equal to\n",
    "$5000/150 = 33$. In python you can run the following codes and make sure\n",
    "the output is 33:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 150\n",
    "### Number of batches :  length dataset / batch size\n",
    "n_batches = horse_x.shape[0] // BATCH_SIZE\n",
    "print(n_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5:** Run the model\n",
    "\n",
    "Last but not least, train the model. You are training the model with 100\n",
    "epochs. That is, the model will see 100 times the images to optimized\n",
    "weights.\n",
    "\n",
    "You are already familiar with the codes to train a model in Tensorflow.\n",
    "The slight difference is to pipe the data before running the training.\n",
    "In this way, the model trains faster.\n",
    "\n",
    "You are interested in printing the loss after ten epochs to see if the\n",
    "model is learning something (i.e., the loss is decreasing). The training\n",
    "takes 2 to 5 minutes, depending on your machine hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "(150, 1024)\n",
      "0 Train MSE: 2919.6782\n",
      "10 Train MSE: 2054.4812\n",
      "20 Train MSE: 1640.9764\n",
      "30 Train MSE: 1438.6802\n",
      "40 Train MSE: 1254.9784\n",
      "50 Train MSE: 1289.5447\n",
      "60 Train MSE: 1529.8217\n",
      "70 Train MSE: 1309.8997\n",
      "80 Train MSE: 1306.5502\n",
      "90 Train MSE: 1254.3444\n",
      "Model saved in path: ./model.ckpt\n"
     ]
    }
   ],
   "source": [
    "## Set params\n",
    "n_epochs = 100\n",
    "\n",
    "## Call Saver to save the model and re-use it later during evaluation\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # initialise iterator with train data\n",
    "    sess.run(iter.initializer, feed_dict={x: horse_x,\n",
    "                                          batch_size: BATCH_SIZE})\n",
    "    print('Training...')\n",
    "    print(sess.run(features).shape) \n",
    "    for epoch in range(n_epochs):       \n",
    "        for iteration in range(n_batches):\n",
    "            sess.run(train)\n",
    "        if epoch % 10 == 0:\n",
    "            loss_train = loss.eval()   # not shown\n",
    "            print(\"\\r{}\".format(epoch), \"Train MSE:\", loss_train) \n",
    "        #saver.save(sess, \"./my_model_all_layers.ckpt\") \n",
    "    save_path = saver.save(sess, \"./model.ckpt\")    \n",
    "    print(\"Model saved in path: %s\" % save_path)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6:** Evaluate the model\n",
    "\n",
    "Now that you have your model trained, it is time to evaluate it. You\n",
    "need to import the test sert from the file `/cifar-10-batches-py/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = unpickle('/Users/Thomas/Downloads/cifar-10-batches-py/test_batch')\n",
    "test_x = grayscale(test_data['data'])\n",
    "#test_labels = np.array(test_data['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: For a Windows machine, the code becomes `test_data = unpickle(r\"E:\\cifar-10-batches-py\\test_batch\")`\n",
    "You can try to print the images 13, which is an horse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_image(test_x[13], shape=[32, 32], cmap = \"Greys_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/thomaspernet/Tensorflow/blob/master/tensorflow/22_autoencoder_v4_files/image004.png)\n",
    "\n",
    "To evaluate the model, you will use the pixel value of this image and\n",
    "see if the encoder can reconstruct the same image after shrinking 1024\n",
    "pixels. Note that, you define a function to evaluate the model on\n",
    "different pictures. The model should work better only on horses.\n",
    "\n",
    "The function takes two arguments:\n",
    "\n",
    "-   `df`: Import the test data\n",
    "\n",
    "-   `image_number`: indicate what image to import\n",
    "\n",
    "The function is divided into three parts:\n",
    "\n",
    "1.  Reshape the image to the correct dimension i.e 1, 1024\n",
    "\n",
    "2.  Feed the model with the unseen image, encode/decode the image\n",
    "\n",
    "3.  Print the real and reconstructed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_image(df, image_number = 1):\n",
    "    ## Part 1: Reshape the image to the correct dimension i.e 1, 1024\n",
    "    x_test = df[image_number]\n",
    "    x_test_1 = x_test.reshape((1, 32*32))\n",
    "    \n",
    "    ## Part 2: Feed the model with the unseen image, encode/decode the image\n",
    "    with tf.Session() as sess:     \n",
    "        sess.run(tf.global_variables_initializer()) \n",
    "        sess.run(iter.initializer, feed_dict={x: x_test_1,\n",
    "                                      batch_size: 1})\n",
    "    ## Part 3:  Print the real and reconstructed image\n",
    "      # Restore variables from disk.\n",
    "        saver.restore(sess, \"./model.ckpt\")  \n",
    "        print(\"Model restored.\")\n",
    "      # Reconstruct image\n",
    "        outputs_val = outputs.eval()\n",
    "        print(outputs_val.shape)\n",
    "        fig = plt.figure()\n",
    "      # Plot real\n",
    "        ax1 = fig.add_subplot(121)\n",
    "        plot_image(x_test_1, shape=[32, 32], cmap = \"Greys_r\")\n",
    "      # Plot estimated\n",
    "        ax2 = fig.add_subplot(122)\n",
    "        plot_image(outputs_val, shape=[32, 32], cmap = \"Greys_r\")\n",
    "        plt.tight_layout()\n",
    "        fig = plt.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the evaluation function is defined, you can have a look of the\n",
    "reconstructed image number thirteen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model.ckpt\n",
      "Model restored.\n",
      "(1, 1024)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAADTCAYAAADUIId2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAGgVJREFUeJzt3cuvnVUZx/GFotIW6b2nF9pyK2CpgPVSGxQaxInEiCbMZMLcP8DoyKF/BCNN1JkR4qBENBZEUWwRD1J7KEg57elp6SmtINTbWNb3p3t5cPNIvp/hk9X3vPu97NWd9cuzLvvHP/7RJEmq5n3v9glIkkScoCRJJTlBSZJKcoKSJJXkBCVJKskJSpJUkhOUJKkkJyhJUklOUJKkki6f5h/7yEc+gm0rbrzxxq527NgxPMaZM2e62k033YRjX3jhhYnP7UMf+lBXu+yyy3DsG2+80dVef/11HHvhwoWutnLlShy7atWqrvbXv/4Vx5I333wT6x/84Acn/ns7d+7EsTMzM10t3aOFhYWulq7lBz7wga6WPsdbb72F9UldunRp4vrll/OrceHCBf4gU3bnnXfiu0TXOV03erbS537f+/r/y9J7kKR7OnL/aezf/va3icem+79ixYqJj5vQdV+9evXEY9N1p04/73//+3HslVdeOfHYEXQMeh5Gfec73/mP75K/oCRJJTlBSZJKcoKSJJXkBCVJKskJSpJU0lRTfK+88grWz58/39VSmowSN9deey2OpRTNkSNHcOxrr73W1VLq5+9//3tXS/tqUQKG/n1rrV1//fVd7dSpUziWUoPpuCltRWmplM6hJBglH1vj+5FSUZTK/Mtf/oJj6Rqn6071kURkSh1WR6nR9FyMoGuX7umf//znZR03pQ7puUj3aWRses9JStuRlKCjFO9IyjW9o3SMdO/puOldGjkuXZ/l7DnoLyhJUklOUJKkkpygJEklOUFJkkqaakjik5/8JNaffvrprnby5EkcSwvzFHBojQMVBw4cwLEbNmzoan/6059w7GOPPdbVUnsWWkxMC4z0mdMCI10H+ryttXb27NmJzy0twNI1vuKKK3Ds+vXrJ/pbrfGiarqWmzdv7mopUPHss892tbT4Tucwshj+bkifZSSUQ8dIi/V0PVJIgp7NFNSh+zcSThppdZQ+G7UCGzlua2PPMd2jFKig80jhJHp309iRENDIdae6IQlJ0nuOE5QkqSQnKElSSU5QkqSSnKAkSSVNNaqU2hdRa5SUzDt37lxXe/LJJ3Hstm3buto111yDY48ePdrVFhcXcSxtDJZQqiVtZEYtn1Ka7IYbbuhqKWGUUlz09xK6Fula0vVJ5/DpT3+6q+3btw/Hbt++vasdPHgQx9Lfm5+fx7H0TKV0YBWpbRN97pTYGklyURIrJbkovTbSkmgkEZsSYvSdks6B0qjpvUtpxJGNF0dSg3RuKT1LKd6UDqR0X0pP0rmlaznSPm0S/oKSJJXkBCVJKskJSpJUkhOUJKmkqYYkZmdnsf7iiy92tbTASAt2acF4aWmpq83NzU183LRQSouc6XyXu0dLWqy/+uqru1paMP7d73438bmlv7dly5aJ/97Fixe72h133IFj77777q42MzODY2lhl8IirbX20Y9+tKsdP34cx37zm9/savQZKkmtjkYWpOn+jRw3vXdUT+dF710KDNC5pWeQFvxHQiEpcJRaB5GRcMqqVatwLLUNW7duHY4dadk1co/oWqbrTn9vOfuR+QtKklSSE5QkqSQnKElSSU5QkqSSnKAkSSVNNcW3sLCAdWqBlDYLpM33UlpmZGMwSp+MtAlJrZkoLZMSYnTclKw5ffp0V7vuuutwbPocFy5c6GrpWtLYW265Bcfed999XW3r1q04ltJLKWn50EMPdTVKM7bW2je+8Y2utnv3bhx76NChrvbwww/j2CrS5pSUJh3ZLHLESKJtJF2Ynld6NkdSfGlsSuCSdM2ovVdK223cuLGr0Wac6Rip1Ro9E6l9EdVTcpHaRr0TqcxJ+AtKklSSE5QkqSQnKElSSU5QkqSSphqSSG1UTp482dXSAuxIuxIKKFCbotZaW7t2bVejdjmt8Z4wjz/+OI6l1kFp0ZCOm64DhUioLUprebH21Vdf7WppwZj2frr33ntx7N69eyc+LrVQOnbsGI6lRdy08E37V6X9yPbv39/VHnvsMRxbxUi7rJEF7fS80buU7im9S6ktEkmfjd79kUBFOi7V0/dECihQ8IFqrbW2Y8eOrrZp06aJ/17aT47uc9q/ir6LU3iLrg+Fplrj+5Gu5ST8BSVJKskJSpJUkhOUJKkkJyhJUklOUJKkkqaa4qMkWGu8kdz58+dxLKW2RjYGSxvy0d9LCaEPf/jDXS0lhDZs2NDVUlskaimS2o9QW5O0IV9KDVKboZT4++xnP9vV9uzZM/G5XXXVVTiWEklr1qzBsefOnetqKU30k5/8pKs98MADOPb666/vaum6V5HuKT2zKZk30uKH7lM6B7p26R2l5yIlz+j9eP311yc+h3RcSr+l55Xe59b4vaG0Xqqnv0ebd45sQpjGUoI3JWKpnpJ59D04kuB8O39BSZJKcoKSJJXkBCVJKskJSpJU0lRDEtTSKBnZwyYtwNLiXloYpv1YZmdncSwtDqd9V2iRMwURnn/++a6WFrhHFiNpcbm11rZt29bVvvKVr+DYffv2dbUrrrgCx9KCOgUyWuN796tf/QrH0memz9Baa3fddVdXS8EHOrcUkKluZI8nCgekNjorV67saunZpOuc9q+i9yY9xxRwWlpawrEUnklj6bOloE7at4naGqVQGAUt0ncChSfSdw0FRlKIiL4HU+iF7ke6R/SOpvOdhL+gJEklOUFJkkpygpIkleQEJUkqyQlKklTSVFN8KU1GSa60yRolh1Lij6SkSkqkEUryfO1rX8OxI+m3n/70p13toYcewrFzc3NdLaWGUjLrwIEDXe3zn/88jqXkUUpmjaS4KHn03HPP4VhKA50+fRrHHj58uKvR5oittTY/P9/VUtuXKlLKjNr5pBQVtatJG/JRPY2lVFy6/5Q6TO8+fbbUNow230sJXroO6XzTdad3L72PI4nIke/Gkc1RacPC1AqK3oXUMo7+3sj389v5C0qSVJITlCSpJCcoSVJJTlCSpJKmGpJIC3a0UJr2XaHWHydOnMCxtJiYFj9pj6fdu3fj2C9+8Ytd7fbbb8exdL4pJEH7EqXWQ0899VRXO3PmDI5Ni6rbt2/vamnhe+3atV0ttS+i+5xa8NACflqspcXWxcVFHHvo0KGuRovTrXHgZDkLu9OQFtXpOqfABy34r1+/HsfSc0HvTDpGGpvuCaH2OmmxnkISaSxdy3R96T1ojfd4SuEkaqOV3lF6PyjgkOppLL2jKURC70I633f6vfEXlCSpJCcoSVJJTlCSpJKcoCRJJTlBSZJKmmqKLyV5KFmX0jLU+iOl166++uqu9qlPfWric0iJtl27dnW1lEaixF5KtFGyJrVWufvuu7taSmul9NKrr7460TmkY6ckz0hKaSTxR8dN7bMeeeSRrnbw4EEcS2nGlGiqYmSzwJSsouc7Pce0IV96R2kTyZT4pHNIKVe6/+nZHtm8j979dH3TdwJ9PkrwtsbPd0quUiuntEHrSJuhdN1ISgISOrd0LSfhLyhJUklOUJKkkpygJEklOUFJkkqaakji2muvxTqFGWixt7XWzp4929W2bt2KY/fv39/V0qLhE0880dVuueUWHEthj7S4TIuGKYgw0iYkLZQSWlxujc85nQPdD1qIbi0vDhMKuKQFXNq3Z+RappZYtHiewhdVjCxyp9DByD5IdIx169bhWHqu0jNBLc3SojqdL7U/ao3v/0jwJX3/0Dmk8SnsQ+/uyJ5dCX2+FL4gKQxBx02BLKq7H5Qk6T3HCUqSVJITlCSpJCcoSVJJTlCSpJKmmuJLrVFoY8CUuKHWP7feeiuOpXTf4cOHcexIOw9KpaRUHY1NyTM6RjruyNiUJqLEVkqvUVrq/PnzOJZSXCml9LOf/ayrnTt3DsfSMRYWFnAsfbaUIp2ZmelqzzzzDI6tYiQZlVJ8dJ9Sio82IUxpu5G2YXRuqSUatcsauQ7pGaRjjLTbSvWR74SU1htJI9J3WHqfKYGbjktjU4qPzjclIifhLyhJUklOUJKkkpygJEklOUFJkkqaakgiLdbSgtvS0hKOpfZDN998M46lFjarV6/+d6f4L1L7IjrfFLKgxdbUpoauT9pHKdVJWqSkz5EWP9PiMKFrkc53bm6uq9E+Va1xuCQt7NJiLbXUao3bwSxnD5t3Ey3Mp3s3EjCga5+OS+eQQgDpO4HQM/ROfLYR6e+N7MO13Ouegg/0nZmCTCPtvWhsCoDQPRppy9Yd77/+l5Ik/Q85QUmSSnKCkiSV5AQlSSrJCUqSVNJUU3yvvfYa1o8cOdLVtm3bhmMpxZdSIpTOSmkiOre0wR21/UntiyjVkjb6G2mXMtLqKCXo6O+lDeBIaklDRjZDS88JXePUvubixYsTH5fSmqtWrcKxVYykOFNqNKVUCaU70zlQoi29H3Tc9BzTs5lSdSmNSkZSZum46fNNKr0f9Byn7w9K4aX3eaTVER1j5HtiZOzb+QtKklSSE5QkqSQnKElSSU5QkqSSphqSeO6557BO7TgefPBBHEshh7T4fdVVV3W1xcVFHEsLjIcOHcKxn/jEJ7ravffei2NpITntYUSLlBs3bsSxk/6tf4cWh8+cOYNj6frcdtttOJbuUWpf9NJLL3W1tAhMC8kpJEGL2WlBfdOmTV1tw4YNOLa6kdABSYEKuvYpGDASTqJnfqTd1jsRvqD3Jp3D6DtGKDSQnnm6Puldonc3tS+ietqHjc5tpEVVeqYm4S8oSVJJTlCSpJKcoCRJJTlBSZJKcoKSJJU01RRfSq9RIiRtskfHSGNpc8KUPqGET9rs6+GHH+5qe/bswbE33nhjV6PkWmutHT9+vKvdfvvtOHbdunVdLV2Hkc0GZ2dnsU4JIWo71Rq30KF2Vq219vjjj3e11BqFElspbUUbRaYUFyWlKAFaCbXAaY2vUfrc9C6k9kf091JCbGTTRKqvWLFi4uOmz0bHTQk82pwyjaXnqjV+ZtPGhPS80aaArXFCOX2P0tiUzKN6OgeS7j2lNd2wUJL0nuMEJUkqyQlKklSSE5QkqSQnKElSSVNN8aW+WZRIO336NI79wx/+0NU+9rGP4dgrr7yyq6UU38imWnNzc13t6NGjOHbz5s1d7fvf//7Ef+v555/H+mc+85muNjMzg2NTIo0SNynFR/2/UpKM0oQpTUSpqLR520iKb2QspTXPnj2LY6sYSVGlNBkl1dJY+nvpHaVefFRLxx1Jo47010vJPNp4M6UOU4qPxqfnjXrTpX6i9Byme0/pwKWlJRxL9yOdb3ofJzWSIn47f0FJkkpygpIkleQEJUkqyQlKklTSVEMS1HqotdZ27tzZ1U6cOIFjqS3J3r17cSwtiq5fv37ic6BARmu8cHn48GEcu2/fvq728ssv49jf/OY3Xe0Xv/gFjj148GBX27VrF47dsmUL1ikwQsdtjReH02emjQzvvPNOHPutb32rq33ve9/DsU8++WRXSwu7tNifFskpJPHCCy/g2CpG2jZRCKA1DpKkMAM9KyMbS6YQAL3nKYgwsrEgWbVqFdbXrFnT1dIGi6kV1EiLH7pHKUR08uTJrpZCEnTd0z2ioAadV2vL3wQz3c9J+AtKklSSE5QkqSQnKElSSU5QkqSSnKAkSSVNNcW3ceNGrFMCJqX4qG0PJV3S2NRGhdKBI5us0cZ7rbX2pS99qatt27YNx1Jib35+HsfSBoIpCUSJndZ448TUjorSj1//+tdxLCUXv/zlL+NYapfzhS98AcfS+abNHyk5lD4bpcNGWl+9G0Y2dUzti+i5SM88JblSqysae+nSJRxL5zYyNqUO6RxGknkp+ZhapVG7trTxIrUfSp+D3o+U4qP3PyVX6flJKT66H+k60D1K130S/oKSJJXkBCVJKskJSpJUkhOUJKmkqYYk0sLj4uJiV0t7kFCLjYWFBRxL7YvS4vKFCxe62p49e3Ds5z73ua724x//GMc+8cQTXY32qWqNP1tqE0KLwNSyp7WxheS0SE6tatJeMzT2jjvuwLHULmnHjh04llo2pZAEGdkXKQUqqkgLzyN779C7kJ4VuqcpcESL7ek5plY86d2nsWlhn8am9jxr167taun6pu8w2i8thSTovUutoEbaF9H7nwInI2Ga5baYSvdoEv6CkiSV5AQlSSrJCUqSVJITlCSpJCcoSVJJU03xUVKuNW6ZktoiUfrk+PHjOJYSN6lNyIsvvtjVNmzYgGM//vGPd7Wf//znOPbZZ5/tatQWpTVuuZLSMlRPLY1GEpEpBTayAeCpU6e62uzsLI6ljQyp7VRrnJT65S9/iWNHEoqbN2/uatQmq5KUtqP3IyXP6HkZafE0kuJLaGxKntFnHkmppWQmPRdpY8L0PtJ5pA1a6Rjpu3HkM9P7mL7vJv33rY21RaJNIZeTiPUXlCSpJCcoSVJJTlCSpJKcoCRJJU01JJFa8dCiYVqwo5YgaRGOFhjTnknULimFDubm5rpaCiLQAux1112HY9esWdPV0vnSAnUKF6TPka7xpGNT6xg6j9SS6Oabb+5qKcywadOmic+Bzjct6tP92L59O46tIrXiGdl7hxa603NM71jaE2hkYZ8W4NP7TC1+Unse+nsp+EDvRzqHkWd+JKCQQi90bimgMBIMoXs3ci1TuzZ6d9N7Nwl/QUmSSnKCkiSV5AQlSSrJCUqSVJITlCSppKmm+EbSZKdPn8ax1O6GNhxrjZMxZ8+exbEjG3hRkicla44ePdrVqL1Pa63t2rWrq6VWPtRSZGZmBsem9ix0jVNKiaTE2DXXXNPVUpqI2lylDQt3797d1Sj52NrYRnh0jem+VZKSjiPPJiW50nHp/qXjUuorvfv0bKb3btLzSscdaYuUEq7pXaLvmnRudOyU4qMkYLruI5sIknTvR1J4dO9G7ufb+QtKklSSE5QkqSQnKElSSU5QkqSSphqSSAuPtDiX2oTQwuPS0hKOnZ+fn/jcKHSQFlVp0W/Lli049tixY13txIkTOHbnzp1d7ciRIziWrlnayyfta3XmzJmulkIS9JnT9aEwQ9qTaMWKFV0tnS+1faJ/31pelJ8UhXH+H9D9S/eU3qXLL+evhJF2ZPSsjAQG0nGpvU7aR4kCIGmxn9oUpbEpGETXLYUWKKyTrjsFItJ7TtdtZF8zCha1xtcnvc/pGP8tf0FJkkpygpIkleQEJUkqyQlKklSSE5QkqaSppvhGNvsa2bAwjaWETzoHSu2kDdko8Zc2IXz00Ue72q9//Wsce+utt3Y12qSvNW5TlFr5UOuh1jiRlDY9pFTlSMoxtWehROM999yDY7/73e92tcXFRRxL9yh9tpHWPFWkz0L1lBAbSZ7RezOSJktpS0r3pc9GUnseah2U3mf6bOm4Cb1L6X0k6R7RczzybKbPMbKxJR0jvft070aShG/nLyhJUklOUJKkkpygJEklOUFJkkqaakgitcFIC26EFmBTWxJa2E+BCjqHNJaOu3fvXhxLLUEOHz6MY2+44YauRm2DWmvt1KlTXS3tu/LKK69gnVrHpBAJLXKndkC///3vu1r6HLSYTZ+tNV50vummm3Dsxo0bu1raN4wWjJezh800pFYzIwv+tDA/sh9UWtin65nCF/SdMNJuKQUfqJ5CHfRsjwQ1WhsLe9BzPNIKKh135cqVXS3tM0X3mf59ksaOtHGahL+gJEklOUFJkkpygpIkleQEJUkqyQlKklTSVFN8Ka1H6ZqRFNVIS5GU5KH0UzqHRx55pKt9+9vfxrFf/epXuxq17GmttT/+8Y9d7a677sKxs7OzWCebN2/GOqWt0maBdC3SZnHUnoVSda1xInJhYQHHUruktFklpZQuXryIY9evX9/VUoKzipFkVEq5UioutSSi+5/eD7p2aSwlSRM6Rkq00WdOSTl6VlI6MKUR6V1KiUi6PmmDVnqX0uegc0vXncbS32qNvzPT80ffo6b4JEnvOU5QkqSSnKAkSSU5QUmSSrosLfr9L6xcuXLiP5bOixb93onPMLK4R2P37NmDY2nhMe3xtG7duq52//3349innnqqq/3gBz/AsWnvFwodpBY658+f72ppgZr2xkqLznTctAhMLZCeeeYZHEvPSQrIjLh06RL3gpqy++67Dx/6kVZHFCQYeeZT6GlkLyUam+7Tct/z9FxRMCS9M+kc6PlO14FCXakN3EjwYaRd28geaPQ50mej75Q09oc//OF/fJf8BSVJKskJSpJUkhOUJKkkJyhJUklOUJKkkqba6iglSiipktIydIy0yd7q1au7WkrnUAontQiipMrJkydx7NatW7taSuzMz893tR07duDY/fv3d7XU/ujEiRNYp1ZFqS0OpfvStaS0VNo08be//W1XS6mfffv2dbXjx4/j2MXFRaxPKj1TVaRU5Mh5j7SgoXuS3lEaO9JyZyStl54Vug7peR0535GNDFPKkb7DUosp2nAw3Tf6e2ksnUO67in9SOj7Yzltw/wFJUkqyQlKklSSE5QkqSQnKElSSVMNSaQF3JG9ZshIS5G0GEn7IKWWRLTwuH379n93iv8i7f1CbX9+9KMf4VhaPE3XNy2U0h5NKcAxslBK+y6l41KQJe3v9fTTT3e1tCfVSLsdetaq7weV0H0auRbpGaL2QyOBjJHrmYIII/sSUX2k1dVoW6XlXp8UqBj5HCMBjkn/VmtjAZkURPlv/X++hZKk9zwnKElSSU5QkqSSnKAkSSU5QUmSSprqhoWSJE3KX1CSpJKcoCRJJTlBSZJKcoKSJJXkBCVJKskJSpJUkhOUJKkkJyhJUklOUJKkkpygJEklOUFJkkpygpIkleQEJUkqyQlKklSSE5QkqSQnKElSSU5QkqSSnKAkSSU5QUmSSnKCkiSV5AQlSSrJCUqSVJITlCSppH8CSD9MBj0jx5cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reconstruct_image(df =test_x, image_number = 13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/thomaspernet/Tensorflow/blob/master/tensorflow/22_autoencoder_v4_files/image005.png)\n",
    "\n",
    "## Summary\n",
    "\n",
    "The primary purpose of an autoencoder is to compress the input data, and\n",
    "then uncompress it into an output that looks closely like the original\n",
    "data.\n",
    "\n",
    "The architecture of an autoencoder symmetrical with a pivot layer named\n",
    "the central layer.\n",
    "\n",
    "You can create the autoencoder using:\n",
    "\n",
    "-   Partial: to create the dense layers with the typical setting:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-   `tf.layers.dense,\n",
    "activation=tf.nn.elu,\n",
    "kernel_initializer=xav_init,\n",
    "kernel_regularizer=l2_regularizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `dense_layer()`: to make the matrix multiplication\n",
    "\n",
    "- you can define the loss function and optimization with:\n",
    "    - `loss = tf.reduce_mean(tf.square(outputs - features)) \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train = optimizer.minimize(loss)`\n",
    "\n",
    "Last run a session to train the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
