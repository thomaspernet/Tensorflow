{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anatomy deep learning\n",
    "\n",
    "A neural network is composed of four principal objects:\n",
    "\n",
    "-   Layers: all the learning occurs in the layers. There are 3 layers\n",
    "    - 1)\n",
    "    Input\n",
    "    - 2) Hidden\n",
    "    - 3) Output\n",
    "\n",
    "-   feature and label: Input data to the network(features) and output\n",
    "    from the network (labels)\n",
    "\n",
    "-   loss function: Metric used to estimate the performance of the\n",
    "    learning phase\n",
    "\n",
    "-   optimizer: Improve the learning by updating the knowledge in the\n",
    "    network\n",
    "\n",
    "A neural network will take the input data and push them into an ensemble\n",
    "of layers. The network needs to evaluate its performance with a loss\n",
    "function. The loss function gives to the network an idea of the path it\n",
    "needs to take before it masters the knowledge. The network needs to\n",
    "improve its knowledge with the help of an optimizer.\n",
    "\n",
    "If you take a look at the figure below, you will understand the\n",
    "underlying mechanism.\n",
    "\n",
    "![](https://github.com/thomaspernet/Tensorflow/blob/master/tensorflow/19_ANN_v9_files/image001.png)\n",
    "\n",
    "The program takes some input values and pushes them into two fully\n",
    "connected layers. Imagine you have a math problem, the first thing you\n",
    "do is to read the corresponding chapter to solve the problem. You apply\n",
    "your new knowledge to solve the problem. There is a high chance you will\n",
    "not score very well. It is the same for a network. The first time it\n",
    "sees the data and makes a prediction, it will not match perfectly with\n",
    "the actual data.\n",
    "\n",
    "To improve its knowledge, the network uses an optimizer. In our analogy,\n",
    "an optimizer can be thought of as rereading the chapter. You gain new\n",
    "insights/lesson by reading again. Similarly, the network uses the\n",
    "optimizer, updates its knowledge, and tests its new knowledge to check\n",
    "how much it still needs to learn. The program will repeat this step\n",
    "until it makes the lowest error possible.\n",
    "\n",
    "In our math problem analogy, it means you read the textbook chapter many\n",
    "times until you thoroughly understand the course content. Even after\n",
    "reading multiple times, if you keep making an error, it means you\n",
    "reached the knowledge capacity with the current material. You need to\n",
    "use different textbook or test different method to improve your score.\n",
    "For a neural network, it is the same process. If the error is far from\n",
    "100%, but the curve is flat, it means with the current architecture; it\n",
    "cannot learn anything else. The network has to be better optimized to\n",
    "improve the knowledge.\n",
    "\n",
    "## The architecture of a neural network\n",
    "\n",
    "**Layers**\n",
    "\n",
    "A layer is where all the learning takes place. Inside a layer, there are\n",
    "an infinite amount of weights (neurons). A typical neural network is\n",
    "often processed by densely connected layers (also called fully connected\n",
    "layers). It means all the inputs are connected to the output.\n",
    "\n",
    "A typical neural network takes a vector of input and a scalar that\n",
    "contains the labels. The most comfortable set up is a binary\n",
    "classification with only two classes: 0 and 1.\n",
    "\n",
    "The network takes an input, sends it to all connected nodes and computes\n",
    "the signal with an **activation** function.\n",
    "\n",
    "![](https://github.com/thomaspernet/Tensorflow/blob/master/tensorflow/19_ANN_v9_files/image002.png)\n",
    "\n",
    "The figure above plots this idea. The first layer is the input values\n",
    "for the second layer, called the hidden layer, receives the weighted\n",
    "input from the previous layer\n",
    "\n",
    "1.  The first node is the input values\n",
    "\n",
    "2.  The neuron is decomposed into the input part and the activation\n",
    "    function. The left part receives all the input from the previous\n",
    "    layer. The right part is the sum of the input passes into an\n",
    "    activation function.\n",
    "\n",
    "3.  Output value computed from the hidden layers and used to make a\n",
    "    prediction. For classification, it is equal to the number of class.\n",
    "    For regression, only one value is predicted.\n",
    "\n",
    "**Activation function**\n",
    "\n",
    "The activation function of a node defines the output given a set of\n",
    "inputs. You need an activation function to allow the network to learn\n",
    "non-linear pattern. A common activation function is a ***Relu, Rectified\n",
    "linear unit.*** The function gives a zero for all negative values.\n",
    "\n",
    "![](https://github.com/thomaspernet/Tensorflow/blob/master/tensorflow/19_ANN_v9_files/image003.png)\n",
    "\n",
    "The other activation functions are:\n",
    "\n",
    "-   Piecewise Linear\n",
    "\n",
    "-   Sigmoid\n",
    "\n",
    "-   Tanh\n",
    "\n",
    "-   Leaky Relu\n",
    "\n",
    "The critical decision to make when building a neural network is:\n",
    "\n",
    "-   How many layers in the neural network\n",
    "\n",
    "-   How many hidden units for each layer\n",
    "\n",
    "Neural network with lots of layers and hidden units can learn a complex\n",
    "representation of the data, but it makes the network's computation very\n",
    "expensive.\n",
    "\n",
    "## Loss function\n",
    "\n",
    "After you have defined the hidden layers and the activation function,\n",
    "you need to specify the loss function and the optimizer.\n",
    "\n",
    "For binary classification, it is common practice to use a binary cross\n",
    "entropy loss function. In the linear regression, you use the mean square\n",
    "error.\n",
    "\n",
    "The loss function is an important metric to estimate the performance of\n",
    "the optimizer. During the training, this metric will be minimized. You\n",
    "need to select this quantity carefully depending on the type of problem\n",
    "you are dealing with.\n",
    "\n",
    "## Optimizer\n",
    "\n",
    "The loss function is a measure of the model's performance. The optimizer\n",
    "will help improve the weights of the network in order to decrease the\n",
    "loss. There are different optimizers available, but the most common one\n",
    "is the Stochastic Gradient Descent.\n",
    "\n",
    "The conventional optimizers are:\n",
    "\n",
    "-   Momentum optimization,\n",
    "\n",
    "-   Nesterov Accelerated Gradient,\n",
    "\n",
    "-   AdaGrad,\n",
    "\n",
    "-   Adam optimization\n",
    "\n",
    "## Limitations of Neural Network\n",
    "\n",
    "**Overfitting**\n",
    "\n",
    "A common problem with the complex neural net is the difficulties in\n",
    "generalizing unseen data. A neural network with lots of weights can\n",
    "identify specific details in the train set very well but often leads to\n",
    "overfitting. If the data are unbalanced within groups (i.e., not enough\n",
    "data available in some groups), the network will learn very well during\n",
    "the training but will not have the ability to generalize such pattern to\n",
    "never-seen-before data.\n",
    "\n",
    "There is a trade-off in machine learning between *optimization* and\n",
    "*generalization*.\n",
    "\n",
    "Optimize a model requires to find the best parameters that minimize the\n",
    "loss of the training set.\n",
    "\n",
    "Generalization, however, tells how the model behaves for unseen data.\n",
    "\n",
    "To prevent the model from capturing specific details or unwanted\n",
    "patterns of the training data, you can use different techniques. The\n",
    "best method is to have a balanced dataset with sufficient amount of\n",
    "data. The art of reducing overfitting is called **regularization**.\n",
    "Let's review some conventional techniques.\n",
    "\n",
    "## Network size\n",
    "\n",
    "A neural network with too many layers and hidden units are known to be\n",
    "highly sophisticated. A straightforward way to reduce the complexity of\n",
    "the model is to reduce its size. There is no best practice to define the\n",
    "number of layers. You need to start with a small amount of layer and\n",
    "increases its size until you find the model overfit.\n",
    "\n",
    "## Weight Regularization\n",
    "\n",
    "A standard technique to prevent overfitting is to add constraints to the\n",
    "weights of the network. The constraint forces the size of the network to\n",
    "take only small values. The constraint is added to the loss function of\n",
    "the error. There are two kinds of regularization:\n",
    "\n",
    "L1: Lasso: Cost is proportional to the absolute value of the weight\n",
    "coefficients\n",
    "\n",
    "L2: Ridge: Cost is proportional to the square of the value of the weight\n",
    "coefficients\n",
    "\n",
    "## Dropout\n",
    "\n",
    "Dropout is an odd but useful technique. A network with dropout means\n",
    "that some weights will be randomly set to zero. Imagine you have an\n",
    "array of weights \\[0.1, 1.7, 0.7, -0.9\\]. If the neural network has a\n",
    "dropout, it will become \\[0.1, 0, 0, -0.9\\] with randomly distributed 0.\n",
    "The parameter that controls the dropout is the dropout rate. The rate\n",
    "defines how many weights to be set to zeroes. Having a rate between 0.2\n",
    "and 0.5 is common.\n",
    "\n",
    "## Example neural network\n",
    "\n",
    "Let's see in action how a neural network works for a typical\n",
    "classification problem. There are two inputs, `x1` and `x2` with a\n",
    "random value. The output is a binary class. The objective is to classify\n",
    "the label based on the two features. To carry out this task, the neural\n",
    "network architecture is defined as following:\n",
    "\n",
    "-   Two hidden layers\n",
    "-   First layer has four fully connected neurons\n",
    "-   Second layer has two fully connected neurons\n",
    "-   The activation function is a Relu\n",
    "-   Add an L2 Regularization with a learning rate of 0.003\n",
    "\n",
    "\n",
    "\n",
    "![](https://media.giphy.com/media/5ZYsQXWwAsHwRLcsOC/giphy.gif)\n",
    "\n",
    "\n",
    "\n",
    "The network will optimize the weight during 180 epochs with a batch size\n",
    "of 10. In the video below you can see how the weights evolve over and\n",
    "how the network improves the classification mapping.\n",
    "\n",
    "First of all, the network assigns random values to all the weights.\n",
    "\n",
    "-   With the random weights, i.e., without optimization, the output loss\n",
    "    is 0.453. The picture below represents the network with different\n",
    "    colors.\n",
    "\n",
    "-   In general, the orange color represents negative values while the\n",
    "    blue colors show the positive values.\n",
    "\n",
    "-   The data points have the same representation; the blue ones are the\n",
    "    positive labels and the orange one the negative labels.\n",
    "\n",
    "![](https://github.com/thomaspernet/Tensorflow/blob/master/tensorflow/19_ANN_v9_files/image005.png)\n",
    "\n",
    "Inside the second hidden layer, the lines are colored following the sign\n",
    "of the weights. The orange lines assign negative weights and the blue\n",
    "one a positive weights\n",
    "\n",
    "As you can see, in the output mapping, the network is making quite a lot\n",
    "of mistake. Let's see how the network behaves after optimization.\n",
    "\n",
    "![](https://github.com/thomaspernet/Tensorflow/blob/master/tensorflow/19_ANN_v9_files/image006.png)\n",
    "\n",
    "The picture below depicts the results of the optimized network. First of\n",
    "all, you notice the network has successfully learned how to classify the\n",
    "data point. You can see from the picture before; the initial weight was\n",
    "-0.43 while after optimization it results in a weight of -0.95.\n",
    "\n",
    "![](https://github.com/thomaspernet/Tensorflow/blob/master/tensorflow/19_ANN_v9_files/image006.png)\n",
    "The idea can be generalized for networks with more hidden layers and\n",
    "neurons. You can play around in the\n",
    "[link](http://playground.tensorflow.org/#activation=relu&regularization=L2&batchSize=10&dataset=gauss&regDataset=reg-plane&learningRate=0.03&regularizationRate=0.003&noise=5&networkShape=4,2&seed=0.65721&showTestData=false&discretize=false&percTrainData=70&x=true&y=true&xTimesY=false&xSquared=fa).\n",
    "\n",
    "# Train a neural network with TensorFlow\n",
    "\n",
    "In this part of the tutorial, you will learn how to train a neural\n",
    "network with TensorFlow using the API's estimator `DNNClassifier`.\n",
    "\n",
    "We will use the MNIST dataset to train your first neural network.\n",
    "Training a neural network with Tensorflow is not very complicated. The\n",
    "preprocessing step looks precisely the same as in the previous\n",
    "tutorials. You will proceed as follow:\n",
    "\n",
    "Step 1: Import the data\n",
    "\n",
    "Step 2: Transform the data\n",
    "\n",
    "Step 3: Construct the tensor\n",
    "\n",
    "Step 4: Build the model\n",
    "\n",
    "Step 5: Train and evaluate the model\n",
    "\n",
    "Step 6: Improve the model\n",
    "\n",
    "**Step 1**: Import the data\n",
    "\n",
    "First of all, you need to import the necessary library. You can import\n",
    "the MNIST dataset using scikit learn.\n",
    "\n",
    "The MNIST dataset is the commonly used dataset to test new techniques or\n",
    "algorithms. This dataset is a collection of 28x28 pixel image with a\n",
    "handwritten digit from 0 to 9. Currently, the lowest error on the test\n",
    "is 0.27 percent with a committee of 7 convolutional neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Thomas/anaconda3/envs/hello-tf/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download scikit learn temporarily at this address. Copy and\n",
    "paste the dataset in a convenient folder. To import the data to python,\n",
    "you can use fetch\\_mldata from scikit learn. Paste the file path\n",
    "inside fetch\\_mldata to fetch the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('/Users/Thomas/Dropbox/Learning/Upwork/tuto_TF/data/mldata/MNIST original')\n",
    "print(mnist.data.shape)\n",
    "print(mnist.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, you import the data and get the shape of both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 784) (56000,) (14000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist.data, mnist.target, test_size=0.2, random_state=42)\n",
    "y_train  = y_train.astype(int)\n",
    "y_test  = y_test.astype(int)\n",
    "batch_size =len(X_train)\n",
    "\n",
    "print(X_train.shape, y_train.shape,y_test.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Transform the data\n",
    "\n",
    "In the previous tutorial, you learnt that you need to transform the data\n",
    "to limit the effect of outliers. In this tutorial, you will transform\n",
    "the data using the min-max scaler. The formula is:\n",
    "\n",
    "$$\n",
    "(X-min_x)/(max_x - min_x)\n",
    "$$\n",
    "\n",
    "Scikit learns has already a function for that: MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## resclae\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "# Train\n",
    "X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\n",
    "# test\n",
    "X_test_scaled = scaler.fit_transform(X_test.astype(np.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: Construct the tensor\n",
    "\n",
    "You are now familiar with the way to create tensor in Tensorflow. You\n",
    "can convert the train set to a numeric column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "      tf.feature_column.numeric_column('x', shape=X_train_scaled.shape[1:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**: Build the model\n",
    "\n",
    "The architecture of the neural network contains 2 hidden layers with 300\n",
    "units for the first layer and 100 units for the second one. We use these\n",
    "value based on our own experience. You can tune theses values and see\n",
    "how it affects the accuracy of the network.\n",
    "\n",
    "To build the model, you use the estimator `DNNClassifier`. You can add\n",
    "the number of layers to the `feature_columns` arguments. You need to set\n",
    "the number of classes to 10 as there are ten classes in the training\n",
    "set. You are already familiar with the syntax of the estimator object.\n",
    "The arguments features columns, number of classes and model\\_dir are\n",
    "precisely the same as in the previous tutorial. The new argument\n",
    "hidden\\_unit controls for the number of layers and how many nodes to\n",
    "connect to the neural network. In the code below, there are two hidden\n",
    "layers with a first one connecting 300 nodes and the second one with 100\n",
    "nodes.\n",
    "\n",
    "To build the estimator, use tf.estimator.DNNClassifier with the\n",
    "following parameters:\n",
    "\n",
    "-   feature\\_columns: Define the columns to use in the network\n",
    "\n",
    "-   hidden\\_units: Define the number of hidden neurons\n",
    "\n",
    "-   n\\_classes: Define the number of classes to predict\n",
    "\n",
    "-   model\\_dir: Define the path of TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'train/DNN', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x10d7b7358>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "estimator = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns,\n",
    "    hidden_units=[300, 100], \n",
    "    n_classes=10, \n",
    "    model_dir = 'train/DNN')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5**: Train and evaluate the model\n",
    "\n",
    "You can use the numpy method to train the model and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into train/DNN/model.ckpt.\n",
      "INFO:tensorflow:loss = 116.52814, step = 1\n",
      "INFO:tensorflow:global_step/sec: 252.89\n",
      "INFO:tensorflow:loss = 12.972335, step = 101 (0.396 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.941\n",
      "INFO:tensorflow:loss = 4.6117616, step = 201 (0.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.762\n",
      "INFO:tensorflow:loss = 12.026966, step = 301 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.205\n",
      "INFO:tensorflow:loss = 24.591938, step = 401 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 282.594\n",
      "INFO:tensorflow:loss = 11.044978, step = 501 (0.354 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.308\n",
      "INFO:tensorflow:loss = 14.145232, step = 601 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.077\n",
      "INFO:tensorflow:loss = 10.733131, step = 701 (0.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 282.853\n",
      "INFO:tensorflow:loss = 15.450101, step = 801 (0.354 sec)\n",
      "INFO:tensorflow:global_step/sec: 198.855\n",
      "INFO:tensorflow:loss = 3.0675507, step = 901 (0.508 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into train/DNN/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 5.3992014.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-04-12:38:39\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from train/DNN/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-04-12:38:40\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.9613571, average_loss = 0.12730463, global_step = 1000, loss = 1782.2649\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: train/DNN/model.ckpt-1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9613571,\n",
       " 'average_loss': 0.12730463,\n",
       " 'loss': 1782.2649,\n",
       " 'global_step': 1000}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the estimator\n",
    "\n",
    "train_input = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X_train_scaled},\n",
    "    y=y_train,\n",
    "    batch_size=50,\n",
    "    shuffle=False,\n",
    "    num_epochs=None)\n",
    "\n",
    "estimator.train(input_fn = train_input,steps=1000) \n",
    "\n",
    "eval_input = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X_test_scaled},\n",
    "    y=y_test, \n",
    "    shuffle=False,\n",
    "    batch_size=X_test_scaled.shape[0],\n",
    "    num_epochs=1)\n",
    "\n",
    "estimator.evaluate(eval_input,steps=None) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current architecture leads to an accuracy on the the evaluation set\n",
    "of 96 percent.\n",
    "\n",
    "**Step 6**: Improve the model\n",
    "\n",
    "You can try to improve the model by adding regularization parameters.\n",
    "\n",
    "We will use an Adam optimizer with a dropout rate of 0.3, L1 of X and L2\n",
    "of y. In TensorFlow, you can control the optimizer using the object\n",
    "train following by the name of the optimizer. TensorFlow is a built-in\n",
    "API for Proximal AdaGrad optimizer.\n",
    "\n",
    "To add regularization to the deep neural network, you can use\n",
    "tf.train.ProximalAdagradOptimizer with the following parameter\n",
    "\n",
    "-   Learning rate: learning\\_rate\n",
    "\n",
    "-   L1 regularization: l1\\_regularization\\_strength\n",
    "\n",
    "-   L2 regularization: l2\\_regularization\\_strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'train/DNN1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1a2c067240>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into train/DNN1/model.ckpt.\n",
      "INFO:tensorflow:loss = 122.88424, step = 1\n",
      "INFO:tensorflow:global_step/sec: 174.996\n",
      "INFO:tensorflow:loss = 19.33489, step = 101 (0.572 sec)\n",
      "INFO:tensorflow:global_step/sec: 137.217\n",
      "INFO:tensorflow:loss = 16.08333, step = 201 (0.732 sec)\n",
      "INFO:tensorflow:global_step/sec: 117.404\n",
      "INFO:tensorflow:loss = 20.75122, step = 301 (0.849 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.17\n",
      "INFO:tensorflow:loss = 31.54268, step = 401 (0.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.682\n",
      "INFO:tensorflow:loss = 21.819052, step = 501 (0.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 235.752\n",
      "INFO:tensorflow:loss = 16.003944, step = 601 (0.424 sec)\n",
      "INFO:tensorflow:global_step/sec: 240.937\n",
      "INFO:tensorflow:loss = 12.829569, step = 701 (0.415 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.855\n",
      "INFO:tensorflow:loss = 24.430027, step = 801 (0.439 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.366\n",
      "INFO:tensorflow:loss = 10.901671, step = 901 (0.448 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into train/DNN1/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 9.81208.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-04-12:38:48\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from train/DNN1/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-04-12:38:48\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.94142854, average_loss = 0.20256661, global_step = 1000, loss = 2835.9326\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: train/DNN1/model.ckpt-1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.94142854,\n",
       " 'average_loss': 0.20256661,\n",
       " 'loss': 2835.9326,\n",
       " 'global_step': 1000}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator_imp = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns,\n",
    "    hidden_units=[300, 100],\n",
    "    dropout=0.3, \n",
    "    n_classes = 10,\n",
    "    optimizer=tf.train.ProximalAdagradOptimizer(\n",
    "      learning_rate=0.01,\n",
    "      l1_regularization_strength=0.01, \n",
    "      l2_regularization_strength=0.01\n",
    "    ),\n",
    "    model_dir = 'train/DNN1')\n",
    "\n",
    "estimator_imp.train(input_fn = train_input,steps=1000) \n",
    "\n",
    "estimator_imp.evaluate(eval_input,steps=None) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values chosen to reduce the over fitting did not improve the model accuracy. Your first model had an accuracy of 96% while the model with L2 regularizer has an accuracy of 95%. You can try with different values and see how it impacts the accuracy.\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial, you learn how to build a neural network. A neural\n",
    "network requires:\n",
    "\n",
    "-   Number of hidden layers\n",
    "\n",
    "-   Number of fully connected node\n",
    "\n",
    "-   Activation function\n",
    "\n",
    "-   Optimizer\n",
    "\n",
    "-   Number of classes\n",
    "\n",
    "In TensorFlow, you can train a neural network for classification problem\n",
    "with:\n",
    "\n",
    "-   `tf.estimator.DNNClassifier`\n",
    "\n",
    "The estimator requires to specify:\n",
    "\n",
    "-   `feature_columns=feature_columns`,\n",
    "\n",
    "-   `hidden_units=[300, 100]`\n",
    "\n",
    "-   `n_classes=10`\n",
    "\n",
    "-   `model_dir`\n",
    "\n",
    "You can improve the model by using different optimizers. In this\n",
    "tutorial, you learned how to use Adam Grad optimizer with a learning\n",
    "rate and add a control to prevent overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
